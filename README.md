## Vision-Based Object Tracking for Unmanned Aircraft Systems

[![license](https://img.shields.io/github/license/mashape/apistatus.svg?maxAge=2592000)](https://github.com/keras-team/keras/blob/master/LICENSE)

![occlusion_handling.gif](./docs/experiment_vids/final_experiments/gifs/occ.gif)

### Overview
One of several key applications of computer vision algorithms is performing navigation tasks with robots, using visual information primarily. This repository consists of code base and documentation of a vision-based object tracking project.
It consists of implementations of experiments for performing diagnosis and analysis of visual tracking techniques under occlusions along with guidance based on a rendezvous cone approach.

### Purpose
Contain computer vision algorithms that may be used in various combinations, pipelines or standalone depending upon the complexity and/or requirement of fulfillment of a task. This repository was created to contain simulations experiments for our paper titled "[Vision-Based Guidance for Tracking Dynamic Objects](https://arxiv.org/abs/2104.09301)"

### Installation
To run the experiments within this repo, essentially `opencv`, `numpy` and `pygame` are needed to be installed along with their dependencies. The `requirements.txt` file (generated by `pip freeze`) may be used as follows.
Navigate into downloaded source folder where `requirements.txt` is located.
Run the following
```python
pip install -r requirements.txt
```

### Usage
From the source folder, navigate into the experiments folder
```bash
cd .\vbot\experiments
```
To run the occlusion handling experiment, run the following
```python
python -m exp_occ
```
To run the lane changing experiment, run the following
```python
python -m exp_lc
```
To run the squircle following experiment, run the following
```python
python -m exp_sf
```

### Running Experiments
The process of running experiments will have the following steps

1. Simulator window pops up
2. User inputs bounding boxing around car (with some extra room)
3. Users hits `space` to start experiment. 
4. Tracker window appears displaying tracking results.
5. To stop the experiment, user selects the simulator window and hits `space`, closes the window.


<!-- 

### Papers

#### Optical Flow
* **1981**
    * [Determining Optical Flow](http://image.diku.dk/imagecanon/material/HornSchunckOptical_Flow.pdf)
    * [An iterative image registration technique with an application to stereo vision](https://cecas.clemson.edu/~stb/klt/lucas_bruce_d_1981_1.pdf)
* **1993**
    * [Good Features to Track](http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf) -->


### License

[MIT](https://github.com/robotic-vision-lab/Vision-Based-Ojbect-Tracking/blob/master/LICENSE)

